---
title: "Big Data (NLp) - part3"
date: 2020-08-11 13:30:00
categories: programming
---
### Tokenizer
```python
from pyspark.ml.feature import Tokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType

#Tokenize word
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
tokenizer
tokenized = tokenizer.transform(dataframe)

#user defined function
def word_list_length(word_list):
   return(len(word_list))
count_tokens = udf(word_list_length, IntegerType())

tokenized.select("sentence","words").withColumn("tokens",count_tokens(col("words"))).show(truncate=False) //count_token is user defined function
```
### Stop words Remover
```python
from pyspark.ml.feature import StopWordsRemover
remover = StopWordsRemover(inputCol="words",outputCol="filtered")
cleaned_df = remover.transform(tokenized).show(truncate=False)
```
### Hashing TF
```python
from pyspark.ml.feature import HashingTF, IDF
hashing = HashingTF(inputCol="filtered",outputCol="hashedvalues", numFeatures=pow(2,4))
hashed_df = hashing.transform(cleaned_df)
```
### Fit the IDF on the data set
```python
idf = IDF(inputCol="hashedvalues",outputCol="features")
idfModel = idf.fit(hashed_df)
rescaled_df = idfModel.transform(hashed_df)
rescaledData.select("filtered","features").show(truncate=False)
```
http://www.tfidf.com/

### Count Vectorizer (count frequency alternative method of hashing TF)
```python
from pyspark.ml.feature import CountVectorizer
cv = CountVectorizer(inputCol="filtered",outputCol="features",vocabSize=N,minDF=2.0)
model=cv.fit(cleaned_df)
result=model.transform(cleaned_df)
```

